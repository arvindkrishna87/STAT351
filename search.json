[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Design and analysis of experiments (R notes)",
    "section": "",
    "text": "Preface\nThese are coding notes for the course STAT351. If you don’t have a background in R, please use the course material on R from the STAT201 course here.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "Rintro.html",
    "href": "Rintro.html",
    "title": "1  R: Introduction",
    "section": "",
    "text": "1.1 Installing R\nGo to the The Comprehensive R Archive Network (CRAN)\nUnder “Download and Install R,” choose “Linux,” “MacOS X” or “Windows.” If you choose Windows, on the next page choose “base,” and on the following page choose “Download R 4.3.1 for Windows” to download the setup program.\nIf you choose MacOS X or Linux you will need to read through the instructions to find the downloads you need for your machine.\nOnce you have downloaded the setup program, execute it and follow the instructions for installing R on your system. If you have an earlier version of R already installed, you may continue to use it, or you can uninstall it and then install the most recent version, which is R 4.3.1.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>R: Introduction</span>"
    ]
  },
  {
    "objectID": "Rintro.html#installing-r",
    "href": "Rintro.html#installing-r",
    "title": "1  R: Introduction",
    "section": "",
    "text": "CRAN",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>R: Introduction</span>"
    ]
  },
  {
    "objectID": "Rintro.html#installing-rstudio",
    "href": "Rintro.html#installing-rstudio",
    "title": "1  R: Introduction",
    "section": "1.2 Installing RStudio",
    "text": "1.2 Installing RStudio\nhttps://rstudio.com/products/rstudio/download/\nChoose your version: RStudio Desktop, Open Source License, Free. It is strongly recommended that you use the latest release of RStudio (v2023.06). After you install RStudio, you can double click on it and open:\n\n\n\nR Studio\n\n\nUsually you will want to import data from a file corresponding to data associated with a homework problem. Such a file will usually end with the extensions *.txt or *.dat. The data files for this course will always be available on the CD that comes with the text and/or on the course web page. A data file will consist of columns of numbers, with nothing separating the columns but “white space.” If each column has a title on top describing what the data in the column represents (e.g., age, weight, income, etc.), we will say that the file has a header.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>R: Introduction</span>"
    ]
  },
  {
    "objectID": "Rintro.html#working-directory",
    "href": "Rintro.html#working-directory",
    "title": "1  R: Introduction",
    "section": "1.3 Working directory",
    "text": "1.3 Working directory\nThe easiest way to import the data into R and have it readily available for the current and future sessions is to first save the data file into your working directory. For example mine is C:\\stat350.\nTo set up the working directory, select the project option by choosing File menu, then New Project, and then Create Project from Existing Directory.\nTo start writing a new R script, navigate to the New File option in the File menu, and select Quarto Document. This will create a *.qmd file. You can write both code and formatted-text in this document. When working on assignment / exam problems, you will work on the *.qmd file, render it as HTML and then submit. You can view some examples on how to write R code and text in a *.qmd file and render it as HTML here.\nFor rough work, i.e., work that won’t be graded, you may use the R script option to write code.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>R: Introduction</span>"
    ]
  },
  {
    "objectID": "Rintro.html#getting-started-with-code",
    "href": "Rintro.html#getting-started-with-code",
    "title": "1  R: Introduction",
    "section": "1.4 Getting started with code",
    "text": "1.4 Getting started with code\n\n1.4.1 Reading data\nSuppose you want to work with the data from Problem 19 of Chapter 1, which is in a file named CH01PR19.txt which you have saved from the CD or the course web page in the Datasets folder within your R working directory. Assume the file has no header. You will want to create a Table object in R containing this data. First choose an appropriate name for the table. Assume you choose to name it Data. Then, you can execute the following code :\n\nData &lt;- read.table(\"./Datasets/CH01PR19.txt\")\n\nThen there will be a Table object in R named Data containing the data in rows and columns. To view it, you would type\n\nData\n\nHowever, if it is a large file, you might not be able to view the whole table at once. In that case, you may use the head() function, which will display only the first 6 rows of Data:\n\nhead(Data)\n\n     V1 V2\n1 3.897 21\n2 3.885 14\n3 3.778 28\n4 2.540 22\n5 3.028 21\n6 3.865 31\n\n\nNote that, in the absence of a header, the columns will be named V1, V2, etc., and the rows will be numbered.\nNow if the file does have a header (which you may have added yourself), you need to change the above command to:\n\nData &lt;- read.table(\"CH01PR19.txt\", header=TRUE)\n\nIn this case, when you view the file you will see the title for each column at the top of each column instead of V1, V2, etc. R regards these titles as names for the columns, and not as data.\nIf you want to load the data file from some other directory, you need to type the full path name in the read.table() command. For instance,\n\nData &lt;- read.table(file=\"C:/stat350/CH01PR19.txt\", header=FALSE) \n\nYou may read data manually as well. Here both Return and New are vectors.\n\nReturn &lt;- c(74,66,81,52,73,62,52,45,62,46,60,46,38)\nNew &lt;-c(5,6,8,11,12,15,16,17,18,18,19,20,20)\n\n\n\n1.4.2 Renaming columns\nNow suppose the file Data has two columns, and the first column is the GPA, while the second column is ACT score. If you would like to rename the columns in your R data table so that each column has a descriptive title, you could give the R command:\n\nnames(Data) &lt;- c(\"GPA\", \"ACT\")\n\nThen when you view the file the titles of the columns will have the new names you assigned:\n\nhead(Data)\n\n    GPA ACT\n1 3.897  21\n2 3.885  14\n3 3.778  28\n4 2.540  22\n5 3.028  21\n6 3.865  31\n\n\nNote that you can also give the columns these titles in the data file before you load it into R, and then use the header = TRUE setting when loading. Also, to avoid errors, you should never include a space in the title of any column\n\n\n1.4.3 Exporting data\nSuppose you wish to export Data to file Intro.csv in your folder.\n\nwrite.table(Data, \"C:/stat350/Intro.csv\", col.names=TRUE, sep=\",\")\n\nSuppose you wish to export Data to Intro.txt with a tab delimiter:\n\nwrite.table(Data, \"C:/stat350/Intro.txt\", col.names=TRUE, sep=\"\\t\")\n\nYou may export R objects to other file types in a similar manner.\n\n\n1.4.4 R environment\nIf you want to see which R objects are currently in your R environment, you can type:\n\nls()\n\nYou may also see these objects at the top right corner of the R Studio interface.\nIf you no longer need one or more of these objects, you can remove them. For instance, if you are done with Data, you can type:\n\nrm(Data)\n\nThen Data will no longer be in your current R environment. When you quit R, if you wish to keep all the new objects in your current R environment, be sure to answer Yes when asked, Save workspace image?\n\n\n1.4.5 Scatter plots and simple linear regression\nSuppose the data for Problem 19 of Chapter One has been stored in an R object named Data which has two columns, the first column named GPA and the second column named ACT. You want to make a scatterplot in R with ACT scores on the horizontal axis and GPA on the vertical axis. The R command is:\n\nplot(Data$ACT, Data$GPA)\n\n\n\n\n\n\n\n\nNote that the dollar sign is used to reference either column in the table named Data. The first argument to the plot() function is the column corresponding to the variable associated with the horizontal axis, and the second argument is the column corresponding to the variable associated with the vertical axis. Alternately, you could define two new vector variables, X and Y, to hold the data of the individual columns, and use these vectors as the arguments to the plot() function:\n\nX &lt;- Data$ACT\nY &lt;- Data$GPA\nplot(X, Y)\n\nFor now we will stick with the former approach. The resulting plot appears in the R Graphics Device within the R interface. Click on it to view it, save it, print it, etc.\nNote that whenever you make a new plot the old one will disappear (this can be changed; but not easily), so save it if you don’t want to lose it. However, the current scatterplot is inadequate. It has no title, the axis labels aren’t very informative, and the points are open circles rather than dark filled-in circles. To fix this, we can add some additional settings to the plot() command:\n\nplot(Data$ACT, Data$GPA, main=\"Problem 1.19\", xlab=\"ACT Test Score\", ylab=\"Freshman GPA\", pch=19)\n\n\n\n\n\n\n\n\nNow we obtain a much nicer scatterplot.\nWhatever you put in quotes after main = will be the title for the plot. Whatever you put in quotes after xlab = and ylab = will the the labels for the horizontal and vertical axes, respectively. The number after pch = is a code for the symbol to use for the points. You can try other numbers from 1 to 25. You can also use any symbol on your keyboard for the points, including numerals and letters, using quotes. For instance, if you want to use an asterisk for the points, type pch=\"*\".\nYou may want to also add a plot of the estimated regression function to the scatterplot of the data. This assumes you have already obtained the least squares estimates of the regression coefficients (see “Simple Linear Regression in R”).\n\nfit &lt;- lm(Data$GPA ~ Data$ACT)\nfit &lt;- lm(GPA~ACT, data=Data)   # another option\nplot(Data$ACT, Data$GPA, main=\"Problem 1.19\", xlab=\"ACT Test Score\", ylab=\"Freshman GPA\", pch=19)\nabline(fit, col = \"red\", lwd = 2) #lwd is for line-width\n\n\n\n\n\n\n\n\nThe line will appear superimposed over the data. You can also just type the actual values for the estimated intercept and slope if you prefer.\nYou may also use ggplot2 to make plots if you wish. ggplot() has a more intuitive syntax as it is based on the Grammar of Graphics, and also has more comprehensive formatting options.\n\nlibrary(ggplot2)\nggplot(Data, aes(x = ACT, y = GPA))+\n  geom_point()+\n  geom_smooth(method = \"lm\")\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n  labs(\n    title = \"Problem 1.19\"\n  )\n\n$title\n[1] \"Problem 1.19\"\n\nattr(,\"class\")\n[1] \"labels\"\n\n\nTo save your plot, click anywhere on the plot, then on the menu bar choose File, then Save as. Choose the format in which you want to save the plot, then where you want to save it on your drive.\nCheck the estimates for the intercept and slope:\n\nfit\n\n\nCall:\nlm(formula = GPA ~ ACT, data = Data)\n\nCoefficients:\n(Intercept)          ACT  \n    2.11405      0.03883  \n\n\nCompute fitted values:\n\nfit$fitted.values \n\nCompute residuals:\n\nfit$residuals\n\nCompute the estimate of \\(\\sigma^2\\), that is, the MSE:\n\nn &lt;- dim(Data)[1]\nsum(fit$residuals^2)/(n-2)\n\n[1] 0.3882848",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>R: Introduction</span>"
    ]
  },
  {
    "objectID": "Rintro.html#formatting-.qmd-file",
    "href": "Rintro.html#formatting-.qmd-file",
    "title": "1  R: Introduction",
    "section": "1.5 Formatting *.qmd file:",
    "text": "1.5 Formatting *.qmd file:\nBefore Quarto, *.Rmd files were commonly used to render HTML files with R code and formatted-text. This Cheatsheet is for formatting *.Rmd files. However, you may use it to format *.qmd files as well.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>R: Introduction</span>"
    ]
  },
  {
    "objectID": "Rintro.html#some-references-about-using-r",
    "href": "Rintro.html#some-references-about-using-r",
    "title": "1  R: Introduction",
    "section": "1.6 Some references about using R:",
    "text": "1.6 Some references about using R:\n\n100 page Introduction to R from the R website http://www.ics.uci.edu/~jutts/st108/R-intro.pdf\nPractical Regression and Anova using R, by Julian Faraway http://cran.r-project.org/doc/contrib/Faraway-PRA.pdf\nR code by Bryan Goodrich for Kutner et al., Applied Linear Statistical Models 5th ed: https://rpubs.com/bryangoodrich",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>R: Introduction</span>"
    ]
  },
  {
    "objectID": "t_test_example.html",
    "href": "t_test_example.html",
    "title": "2  Hypothesis testing examples",
    "section": "",
    "text": "2.1 t-test",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Hypothesis testing examples</span>"
    ]
  },
  {
    "objectID": "t_test_example.html#t-test",
    "href": "t_test_example.html#t-test",
    "title": "2  Hypothesis testing examples",
    "section": "",
    "text": "2.1.1 Question\nCola manufacturers want to test how much the sweetness of a new cola drink is affected by storage. The sweetness loss due to storage was evaluated by 10 professional tasters (by comparing the sweetness before and after storage):\nTaster          Sweetness loss\n\n 1         2.0\n 2         0.4\n 3         0.7  \n 4         2.0  \n 5       −0.4   \n 6         2.2  \n 7       −1.3   \n 8         1.2  \n 9         1.1\n10         2.3\nObviously, we want to test if storage results in a loss of sweetness\nLet \\(\\mu\\) denote the sweetness loss, thus:\nNull hypothesis: \\(H_0: \\mu = 0\\)\nAlternate hypothesis: \\(H_a: \\mu &gt; 0\\)\n\n\n2.1.2 Solution\nSample mean (\\(\\bar{x}\\)):\n\ndata &lt;- c(2, 0.4, 0.7, 2, -0.4, 2.2, -1.3, 1.2, 1.1, 2.3)\n\nxbar &lt;- mean(data)\nxbar\n\n[1] 1.02\n\n\nT-statistic:\n\nt = xbar/(sd(data)/sqrt(10))\nt\n\n[1] 2.696689\n\n\np-value:\n\n1-pt(t, df = 9)\n\n[1] 0.01226316\n\n\nIf the probability of Type I error considered is 5%, then we reject the null hypothesis, and conclude that the sweetness loss is indeed greater than 0.\nIf the probability of Type I error considered is 1%, then we fail to reject the null hypothesis, and conclude that the sweetness loss is indeed 0.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Hypothesis testing examples</span>"
    ]
  },
  {
    "objectID": "t_test_example.html#two-sample-t-test",
    "href": "t_test_example.html#two-sample-t-test",
    "title": "2  Hypothesis testing examples",
    "section": "2.2 Two-sample t-test",
    "text": "2.2 Two-sample t-test\n\n2.2.1 Question\nIn a study of lettuce growth, ten seedlings were randomly allocated to be grown in either a standard nutrient solution or in a solution containing extra nitrogen. After 22 days, the plants were harvested and weighed. The table below summarizes the results. Can we conclude that extra nitrogen enhances growth?\n\n\n\nNutrient solution\nn\nmean\nSD\n\n\n\n\nStandard\n5\n3.62\n0.54\n\n\nExtra\n5\n4.17\n0.67\n\n\n\n\n\n2.2.2 Solution\nWe will first test the hypothesis if the variance of the responses corresponding to the two treatments are the same or not.\nWe will assume that the response follows the normal distribution.\nLet \\(\\sigma_{standard}^2\\) denote the variance of the observations treated with standard solution, \\(\\sigma_{extra}^2\\) denote the variance of the observations treated with extra nitrogen. Then,\nNull hypothesis: \\(\\sigma_{standard}^2 = \\sigma_{extra}^2\\)\nAlternate hypothesis: \\(\\sigma_{standard}^2 \\ne \\sigma_{extra}^2\\)\n\n# F-statistic:\nF = (0.54/0.67)^2\nF\n\n[1] 0.6495879\n\n\n\n# Critical values based on a significance level of 5%\nleft_tail_critical_value &lt;- qf(0.025, 4, 4)\nleft_tail_critical_value\n\n[1] 0.1041175\n\n\n\nright_tail_critical_value &lt;- qf(0.975, 4, 4)\nright_tail_critical_value\n\n[1] 9.60453\n\n\nAs the \\(F\\)-statistic is between the critical values, we do not reject the null hypothesis.\nThus, we will use the pooled-variance to conduct a 2-sample t-test for equality of means.\nLet \\(\\mu_{standard}\\) denote the mean growth with the standard solution, \\(\\mu_{extra}\\) denote the mean growth with extra nitrogen. Then,\nNull hypothesis: \\(\\mu_{standard} = \\mu_{extra}\\)\nAlternate hypothesis: \\(\\mu_{standard} \\ne \\mu_{extra}\\)\n\n# Pooled variance\nsp2 = (0.54^2 + 0.67^2)/2\n\n# t-statistic\nt &lt;- (3.62 - 4.17)/(sqrt(sp2*2/5))\n\n#p-value\n2*pt(t, 8)\n\n[1] 0.1908168\n\n\nAs the \\(p\\)-value is high, we do not reject the null hypothesis that the extra nitrogen does not enhance growth.\nAlternatively, the Welch’s test for unequal variances can be used without testing for equality of variances in the two samples.\n\nn = 5; s1 = 0.54; s2 = 0.67\nnu = (n-1)*(((s1^2 + s2^2)/n)^2)/(((s1^2)/n)^2+((s2^2)/n)^2)\nt0 &lt;- (3.62 - 4.17)/(sqrt((s1^2+s2^2)/5))\n\n2*pt(t0, 7.654594)\n\n[1] 0.1924672\n\n\nAs the power of Welch’s t-test is similar to that of Student’s t-test, even when the population variances are equal and sample sizes are balanced, Welch’s test can always be used, irrespective of the variances being equal or not.\nHowever, if the variances are unequal, then Student’s t-test must not be used. Type 1 error rate will be higher for a Student’s t-test, particularly if one of the samples has a relatively higher variance, and a smaller sample size as compared to the other sample.\nThe functions t.test() and power.t.test() can be used to conveniently test the hypothesis, estimate confidence intervals, estimate power of the test, or the observations needed for a certain power of the test. Look at the documentation of these function, by executing the code ?t.test() or ?power.t.test() in the R console, to see the parameters you need to specify for using them.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Hypothesis testing examples</span>"
    ]
  },
  {
    "objectID": "ANOVA.html",
    "href": "ANOVA.html",
    "title": "3  One way ANOVA",
    "section": "",
    "text": "3.1 ANOVA Table\nLet us consider an example where we have results from an experiment to compare yields (as measured by dried weight of plants) obtained under a control and two different treatment conditions. The dataset is named as PlantGrowth and it can be found in the library car.\n# Loading the dataset\nlibrary(car)\nlibrary(DescTools)\ndata &lt;- PlantGrowth\nLet us print the anova table.\nanova(lm(weight ~ group, data = data))\n\nAnalysis of Variance Table\n\nResponse: weight\n          Df  Sum Sq Mean Sq F value  Pr(&gt;F)  \ngroup      2  3.7663  1.8832  4.8461 0.01591 *\nResiduals 27 10.4921  0.3886                  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\nThe null hypothesis that all treatments are the same is rejected at a significance level of 5%.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>One way ANOVA</span>"
    ]
  },
  {
    "objectID": "ANOVA.html#confidence-interval",
    "href": "ANOVA.html#confidence-interval",
    "title": "3  One way ANOVA",
    "section": "3.2 Confidence interval",
    "text": "3.2 Confidence interval\nLet us print the Bonferroni’s confidence intervals:\n\naov_object &lt;- aov(lm(weight ~ group, data = data))\nPostHocTest(aov_object, method = \"bonferroni\")\n\n\n  Posthoc multiple comparisons of means : Bonferroni \n    95% family-wise confidence level\n\n$group\n            diff     lwr.ci    upr.ci   pval    \ntrt1-ctrl -0.371 -1.0825786 0.3405786 0.5832    \ntrt2-ctrl  0.494 -0.2175786 1.2055786 0.2630    \ntrt2-trt1  0.865  0.1534214 1.5765786 0.0134 *  \n\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nBased on Bonferroni’s method, we observe that treatment 1 and treatment 2 are significantly different, but there is no difference between other pairs.\nBonferroni’s confidence intervals are overly conservative. Let us find the confidence intervals basead on Tukey’s method:\n\nPostHocTest(aov_object, method = \"hsd\")\n\n\n  Posthoc multiple comparisons of means : Tukey HSD \n    95% family-wise confidence level\n\n$group\n            diff     lwr.ci    upr.ci   pval    \ntrt1-ctrl -0.371 -1.0622161 0.3202161 0.3909    \ntrt2-ctrl  0.494 -0.1972161 1.1852161 0.1980    \ntrt2-trt1  0.865  0.1737839 1.5562161 0.0120 *  \n\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>One way ANOVA</span>"
    ]
  },
  {
    "objectID": "ANOVA.html#model-assumptions-check",
    "href": "ANOVA.html#model-assumptions-check",
    "title": "3  One way ANOVA",
    "section": "3.3 Model assumptions check",
    "text": "3.3 Model assumptions check\nThe conclusions of the statistical tests and the confidence intervals are based on the assumption that random error is normally distributed with mean 0 and constant variance, i.e., \\(\\epsilon_{ij} \\sim N(0, \\sigma^2)\\).\nThe diagnostic plots and statistical tests for checking model assumptions can be found here.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>One way ANOVA</span>"
    ]
  },
  {
    "objectID": "ANOVA.html#contrasts",
    "href": "ANOVA.html#contrasts",
    "title": "3  One way ANOVA",
    "section": "3.4 Contrasts",
    "text": "3.4 Contrasts\n\n3.4.1 Class comparison\nThis is an example to use orthogonal contrasts to answer questions of interest. The data rice_seed_data.csv consists of the shoot dry weight (in mg) of an experiment to determine the effect of seed treatment by acids on the early growth of rice seeds.\nThe investigator had several specific questions in mind from the beginning: -Do acid treatments affect seedling growth? - Is the effect of organic acids different from that of inorganic acids? - Is there a difference in the effects of the two different organic acids?\nWe will create orthogonal contrasts, and perform an ANOVA analysis to answer these questions.\n\nrice_data &lt;- read.csv('rice_seed_data.csv')\nanova(lm(growth~treatment, data = rice_data))\n\nAnalysis of Variance Table\n\nResponse: growth\n          Df  Sum Sq  Mean Sq F value    Pr(&gt;F)    \ntreatment  3 0.87369 0.291232  33.874 3.669e-07 ***\nResiduals 16 0.13756 0.008597                      \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nc1 &lt;- c(3, -1, -1, -1)\nc2 &lt;- c(0, -2, 1, 1)\nc3 &lt;- c(0, 0, 1, -1)\n\n# Contrast matrix\nmat.contrast &lt;- cbind(c1, c2, c3)\ncolnames(mat.contrast) &lt;- paste0(\"c\",1:3)\n\n# Converting treatment to a factor variable\nrice_data$treatment &lt;- as.factor(rice_data$treatment)\ncontrasts(rice_data$treatment) &lt;- mat.contrast\n\n# ANOVA table\nmodel &lt;- aov(growth ~ treatment, data = rice_data,\n                   contrasts = list(treatment = mat.contrast))\n\n# Splitting the Treatment sum of squares into independent \n# orthogonal contrast components\nsummary.aov(model,split = list(treatment = list(\"Control vs acid\"=1,  \n                              \"Inorganic vs organic\" = 2, 'Between organic'=3)))\n\n                                  Df Sum Sq Mean Sq F value   Pr(&gt;F)    \ntreatment                          3 0.8737  0.2912  33.874 3.67e-07 ***\n  treatment: Control vs acid       1 0.7415  0.7415  86.244 7.61e-08 ***\n  treatment: Inorganic vs organic  1 0.1129  0.1129  13.126  0.00229 ** \n  treatment: Between organic       1 0.0194  0.0194   2.252  0.15293    \nResiduals                         16 0.1376  0.0086                     \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nWe conclude that there is no statistically significant difference between the effect of organic acids. However, there is a statistically significant difference between the effect of inorganic and organic acids, and an even higher statistically significant difference between the weight of seeds in the control group and the weight seeds treated with acid.\n\n\n3.4.2 Trend comparison\nThis is an example to identify if there is a linear, quadratic, or higher order relationship between a continuous treatment and the response.\nConsider the data laser_power.csv, which consists of laser power, and the corresponding strength of the material obtained using a machining process involving the laser. The question to be answered is if there is a linear or quadratic relationship between the laser power and strength of the material.\n\ndata &lt;- read.csv('laser_power.csv')\ndata$power &lt;- as.factor(data$power)\n\n# Linear contrast\nc1 &lt;- c(-1, 0, 1)\n\n# Quadratic contrast\nc2 &lt;- c(1, -2, 1)\n\nmat.contrast &lt;- cbind(c1, c2)\nmodel &lt;- aov(strength ~ power, data = data,\n             contrasts = list(power = mat.contrast))\nsummary.aov(model, split = list(power = list(\"linear\" = 1,\n                                                 \"quadratic\" = 2)))\n\n                   Df Sum Sq Mean Sq F value  Pr(&gt;F)   \npower               2 224.18  112.09  11.318 0.00920 **\n  power: linear     1 223.75  223.75  22.593 0.00315 **\n  power: quadratic  1   0.44    0.44   0.044 0.84083   \nResiduals           6  59.42    9.90                   \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nWe conclude that there is a linear relationship between laser power and strength. This can be seen visually as well as below.\n\ndata$power &lt;- as.integer(substr(data$power,5,8))\nplot(data$power, data$strength)\nabline(lm(strength~power, data = data))",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>One way ANOVA</span>"
    ]
  },
  {
    "objectID": "Multiple_factors.html",
    "href": "Multiple_factors.html",
    "title": "4  Multiple factors",
    "section": "",
    "text": "4.1 Randomized complete block design (RCBD)\nLet us consider an example, where the shear strength of steel plate girders needs to be modeled as a function of the four methods (treatment) and nine girders (blocks).\nlibrary(ACSWR)\nlibrary(reshape2)\nlibrary(DescTools)\nlibrary(lmtest)\n\n# Visualizing treatment effects\ndata(\"girder\")\nboxplot(girder[,2:5])\n\n\n\n\n\n\n\n# Melting data to make it suitable to fit a linear regression model\n# using the 'lm' function\ndata_melt &lt;- melt(girder, variable.name = \"treatment\", id = 'Girder')\nanova(lm(value~Girder+treatment, data = data_melt))\n\nAnalysis of Variance Table\n\nResponse: value\n          Df  Sum Sq Mean Sq F value    Pr(&gt;F)    \nGirder     8 0.08949 0.01119  1.6189    0.1717    \ntreatment  3 1.51381 0.50460 73.0267 3.296e-12 ***\nResiduals 24 0.16584 0.00691                      \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\nThe null hypothesis that there is no effect of the method on the sheer strength of the girder is rejected.\nLet us compare the methods pairwise.\naov_object &lt;- aov(value~Girder+treatment, data = data_melt)\n\n# Tukey's method\nPostHocTest(aov_object, method = \"hsd\")$treatment\n\n                        diff       lwr.ci      upr.ci         pval\nKarisruhe-Aarau    0.5452222  0.437124171  0.65332027 3.225975e-12\nLehigh-Aarau       0.2713333  0.163235283  0.37943138 2.106339e-06\nCardiff-Aarau      0.1106667  0.002568616  0.21876472 4.343575e-02\nLehigh-Karisruhe  -0.2738889 -0.381986940 -0.16579084 1.808447e-06\nCardiff-Karisruhe -0.4345556 -0.542653606 -0.32645750 3.675706e-10\nCardiff-Lehigh    -0.1606667 -0.268764717 -0.05256862 2.164599e-03\nAll methods are different from each other according to Tukey’s method (at 5% significance level).\n# Bonferroni's method\nPostHocTest(aov_object, method = \"bonferroni\")$treatment\n\n                        diff       lwr.ci      upr.ci         pval\nKarisruhe-Aarau    0.5452222  0.432559601  0.65788484 3.308011e-12\nLehigh-Aarau       0.2713333  0.158670712  0.38399595 2.208198e-06\nCardiff-Aarau      0.1106667 -0.001995955  0.22332929 5.631958e-02\nLehigh-Karisruhe  -0.2738889 -0.386551510 -0.16122627 1.894578e-06\nCardiff-Karisruhe -0.4345556 -0.547218177 -0.32189293 3.770501e-10\nCardiff-Lehigh    -0.1606667 -0.273329288 -0.04800405 2.453939e-03\nAll methods except Aarau and Cardiff are different from each other according to Bonferroni’s method (at 5% significance level).\nLet us verify if the model assumptions are satisfied.\npar(mfrow = c(2,2))\nmodel &lt;- lm(value~Girder+treatment, data = data_melt)\nplot(model)\nshapiro.test(model$residuals)\n\n\n    Shapiro-Wilk normality test\n\ndata:  model$residuals\nW = 0.94966, p-value = 0.102\nThe errors are normally distribued.\nbptest(model)\n\n\n    studentized Breusch-Pagan test\n\ndata:  model\nBP = 22.205, df = 11, p-value = 0.02283\nThe error variance assumption is also satisfied at a 1% significance level. There is no strong deviation from the assumption.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Multiple factors</span>"
    ]
  },
  {
    "objectID": "Multiple_factors.html#two-way-layout-fixed-effects",
    "href": "Multiple_factors.html#two-way-layout-fixed-effects",
    "title": "4  Multiple factors",
    "section": "4.2 Two-way layout: Fixed effects",
    "text": "4.2 Two-way layout: Fixed effects\nA manufacturer was interested in finding differences in torque values of a lock-nut. The two factors effecting the torque are the type of plating, and whether the locknut is threaded into a bolt or a mandrel. We’ll use two-way ANOVA to find if the two factors or their interaction effect the torque.\n\ndata &lt;- read.table('bolt.dat.txt', header = TRUE)\ndata_melt &lt;- melt(data, variable.name = 'plating', value.name = 'torque')\n\nUsing M.B as id variables\n\nhead(data_melt)\n\n  M.B plating torque\n1   M     P.O     10\n2   M     P.O     13\n3   M     P.O     17\n4   M     P.O     16\n5   M     P.O     15\n6   M     P.O     14\n\nanova(lm(torque ~ M.B*plating, data = data_melt))\n\nAnalysis of Variance Table\n\nResponse: torque\n            Df Sum Sq Mean Sq F value    Pr(&gt;F)    \nM.B          1  821.4  821.40 22.4563 1.604e-05 ***\nplating      2 2290.6 1145.32 31.3118 9.363e-10 ***\nM.B:plating  2  665.1  332.55  9.0916 0.0003952 ***\nResiduals   54 1975.2   36.58                      \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n#data['plating_bolt'] = apply(data_melt[,1:2], axis = 1, function(x) paste)\n\nThe two factors and their interaction significantly effects the torque.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Multiple factors</span>"
    ]
  },
  {
    "objectID": "Multiple_factors.html#two-way-layout-random-effects",
    "href": "Multiple_factors.html#two-way-layout-random-effects",
    "title": "4  Multiple factors",
    "section": "4.3 Two-way layout: Random effects",
    "text": "4.3 Two-way layout: Random effects\nTen food items are being tested by 5 judges (operators) for quality. Each judge inspects a food item 3 times, and gives a score. Answer the following questions with appropriate analysis:\n\nIs there a statistically significant variation in the scores given by judges for the same quality of food? If yes, quantify the variation.\nIs the variation in the scores given by judges for the same quality of food dependent on the food item?\nIs there a statistically significant variation in the quality of the food items after removing the variation in scores due to different judges? If yes, quantify the variation.\n\n\ndata &lt;- read.csv('sensory_data.csv', header = TRUE)\nhead(data)\n\n   Item Operator1 Operator2 Operator3 Operator4 Operator5\n1 Item1       4.3       4.9       3.3       5.3       4.4\n2 Item1       4.3       4.5       4.0       5.5       3.3\n3 Item1       4.1       5.3       3.4       5.7       4.7\n4 Item2       6.0       5.3       4.5       5.9       4.7\n5 Item2       4.9       6.3       4.2       5.5       4.9\n6 Item2       6.0       5.9       4.7       6.3       4.6\n\ndata_melt &lt;- melt(data, variable.name = \"Operator\", value.name = 'property')\n\nUsing Item as id variables\n\nhead(data_melt)\n\n   Item  Operator property\n1 Item1 Operator1      4.3\n2 Item1 Operator1      4.3\n3 Item1 Operator1      4.1\n4 Item2 Operator1      6.0\n5 Item2 Operator1      4.9\n6 Item2 Operator1      6.0\n\nanova(lm(property~Item*Operator, data = data_melt))\n\nAnalysis of Variance Table\n\nResponse: property\n               Df Sum Sq Mean Sq  F value    Pr(&gt;F)    \nItem            9 612.40  68.044 199.3084 &lt; 2.2e-16 ***\nOperator        4  25.49   6.372  18.6643 1.739e-11 ***\nItem:Operator  36  12.97   0.360   1.0549     0.406    \nResiduals     100  34.14   0.341                       \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nYes, there is a statistically significant variation in the scores given by judges for the same quality of food\nThe standard deviation in the scores given by judges for the same quality of food is:\n\nsqrt((6.37 - 0.36)/30)\n\n[1] 0.4475861\n\n\nThe variation in the scores given by judges for the same quality of food does not depend on the food item\nYes, there is a statistically significant variation in the quality of the food items after removing the variation in scores due to different judges.\nThe standard deviation in the scores given food items after removing the variation due to different judges is:\n\nsqrt((68.04 - 0.36)/15)\n\n[1] 2.124147",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Multiple factors</span>"
    ]
  },
  {
    "objectID": "Computer_model.html",
    "href": "Computer_model.html",
    "title": "5  Computer model",
    "section": "",
    "text": "5.1 Polynomial interpolation\nAssume that the true computer model is the following:\nWe will consider potential methods to replace the computer model above with a metamodel or a surrogate model.\nBelow is an example of a \\(7\\)-point polynomial interpolator.\ncurve(1/(1+x^2), from=-4, to=4, ylim=c(-.7,1))\nx=seq(-4,4,length=7)\ny=1/(1+x^2)\nX=as.matrix(cbind(1,x,x^2,x^3,x^4,x^5,x^6))\na=solve(X,y)\nu=seq(-4,4,length=100)\nyhat=u\nfor(i in 1:100)\n  yhat[i]=sum(c(1,u[i],u[i]^2,u[i]^3,u[i]^4,u[i]^5,u[i]^6)*a)\nlines(u,yhat, col=2, lty=2, lwd = 2)\npoints(x,y,col=2)\nNotice that the polynomial interpolation model tends to be unstable near the edges, this is called Runge’s phenomenon.\nThe instability increases as the degree of the polynomial increases. Consider the same example if 9 equally-spaced points are considered, instead of 7.\ncurve(1/(1+x^2), from=-4, to=4, ylim=c(-.7,1))\nx=seq(-4,4,length=9)\ny=1/(1+x^2)\nX=as.matrix(cbind(1,x,x^2,x^3,x^4,x^5,x^6,x^7,x^8))\na=solve(X,y)\nu=seq(-4,4,length=100)\nyhat=u\nfor(i in 1:100)\n  yhat[i]=sum(c(1,u[i],u[i]^2,u[i]^3,u[i]^4,u[i]^5,u[i]^6, u[i]^7,u[i]^8)*a)\nlines(u,yhat, col='blue', lty=3, lwd = 2)\npoints(x,y,col=\"blue\")",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Computer model</span>"
    ]
  },
  {
    "objectID": "Computer_model.html#splines",
    "href": "Computer_model.html#splines",
    "title": "5  Computer model",
    "section": "5.2 Splines",
    "text": "5.2 Splines\nLet us consider splines as the metamodel to replace the same computer model considered above.\n\ncurve(1/(1+x^2), from=-4, to=4, ylim=c(-.7,1))\n\n# Splines with 7 points\n\nx=seq(-4,4,length=7)\ny=1/(1+x^2)\npoints(x,y,col=2)\na=splinefun(x,y,method=\"natural\")\ncurve(a,add=T,col=2, lwd = 2)\n\n# Splines with 9 points\n\nx=seq(-4,4,length=9)\ny=1/(1+x^2)\npoints(x,y, pch=3,col=3)\na=splinefun(x,y,method=\"natural\")\ncurve(a,add=T,col=3, lty=2, lwd = 2)\n\n\n\n\n\n\n\n\nWe observe that splines fit the points smoothly, which is desired. However, splines do not scale up and are difficult to fit in case of higher dimensions.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Computer model</span>"
    ]
  },
  {
    "objectID": "Computer_model.html#random-functions",
    "href": "Computer_model.html#random-functions",
    "title": "5  Computer model",
    "section": "5.3 Random functions",
    "text": "5.3 Random functions\nWe will now adopt a statistical approach to replace the computer model with a metamodel, where we view the deterministic computer model as a realization from a stochastic process.\nLet us consider quadratic functions as the random functions to develop the metamodel.\n\nx=seq(-1,1,length=10)\nb=rnorm(3)\ny=b[1]+b[2]*x+b[3]*x^2\nplot(x,y,\"l\", ylim=c(-3,3))\nb=rnorm(3)\ny=b[1]+b[2]*x+b[3]*x^2\nlines(x,y,\"l\")\nfor(i in 1:10)\n{b=rnorm(3)\ny=b[1]+b[2]*x+b[3]*x^2\nlines(x,y,\"l\",col=i)}\n\n\n\n\n\n\n\n\nWe want a more flexible random function, so that a flexible computer model could correspond to a realization from that random function.\nSuppose \\(y(x_i) \\sim N(0, \\sigma^2)\\). Let us plot a realization of this model for \\(\\sigma^2 = 1\\).\n\nN=100\nx=seq(-1,1,length=N)\ny=rnorm(N)\nplot(x,y,\"l\", ylim=c(-3,3))\n\n\n\n\n\n\n\n\nAlthough the function does seem to be flexible, it is not smooth. We want to have a smooth function, which is also flexible, as the metamodel.\nThe reason why the above realization of a computer model is not smooth is because the response values are not correlated. The sample paths for the above random function follow the multivariate normal distribution with the identity matrix as the covariance matrix, i.e.,\n\\[\\textbf{y} \\sim N_{100}(\\textbf{0}, \\textbf{I}),\\]\nwhere \\(\\textbf{I}\\) is the \\(100 \\times 100\\) identity matrix.\nFor smooth functions, adjacent points \\(y(x)\\) and \\(y(x + \\Delta)\\) should be positively correlated.\nLet us introduce correlation between response values to make the realizations of the computer models more smooth.\nLet us assume that the response values have an underlying multivariate normal distribution, with a covariance matrix that is not the diagonal matrix.\nFor example, consider the covariance matrix \\(R\\). where:\n\\[\\textbf{R} = \\exp\\{-\\theta(x_i-x_j)^2\\}_{100 \\times 100}\\] In the above correlation function, we can control the flexibility by changing the value of \\(\\theta\\). The higher the value of \\(\\theta\\), the lesser is the correlation between the response values, for a given distance between inputs, and the higher is the flexibility of the function. Thus, this correlation function can provide the desired level of flexibility while also providing smoothness in the function.\nThen,\n\\[\\textbf{y} \\sim N(0, \\textbf{R})\\] Let us visualize 10 realizations from the above random function to see the smoothness.\nIf \\(\\textbf{z} \\sim N(\\textbf{0}, \\textbf{I})\\), then \\(\\textbf{y} = \\textbf{R}^{1/2}\\textbf{z}\\).\nProof: Given \\(y = \\textbf{R}^{1/2}\\textbf{z}\\),\n\\(E(\\textbf{y}) = \\textbf{R}^{1/2}E(\\textbf{z}) = 0\\)\n\\(Var(\\textbf{y}) = Var(\\textbf{R}^{1/2}\\textbf{z})\\)\n\\(\\implies Var(\\textbf{y}) = \\textbf{R}^{1/2}Var(\\textbf{z})R^{1/2})\\)\n(Refer to equation 313 in the matrix cookbook)\n\\(\\implies Var(\\textbf{y}) = \\textbf{R}^{1/2}\\textbf{I}\\textbf{R}^{1/2})\\)\n\\(\\implies Var(\\textbf{y}) = \\textbf{R}\\)\n\nE=as.matrix(dist(x, diag=T, ,upper=T))\n\n# Correlation matrix R\nR=exp(-0.5*E^2)\neig=eigen(R)\nR.sqrt=eig$vec%*%diag(sqrt(eig$val+10^(-10)))%*%t(eig$vec)\nz=rnorm(N)\ny=R.sqrt%*%z\nplot(x,y,\"l\", ylim=c(-3,3))\nfor(i in 1:10)\n  lines(x,R.sqrt%*%rnorm(N),col=i)\n\n\n\n\n\n\n\n\nWe can increase the flexibility by increasing the value of the correlation parameter \\(\\theta\\) to say \\(\\theta = 5\\), as shown below.\n\nE=as.matrix(dist(x, diag=T, ,upper=T))\n\n# Correlation matrix R\nR=exp(-5*E^2)\neig=eigen(R)\nR.sqrt=eig$vec%*%diag(sqrt(eig$val+10^(-10)))%*%t(eig$vec)\nz=rnorm(N)\ny=R.sqrt%*%z\nplot(x,y,\"l\", ylim=c(-3,3))\nfor(i in 1:10)\n  lines(x,R.sqrt%*%rnorm(N),col=i)",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Computer model</span>"
    ]
  },
  {
    "objectID": "Computer_model.html#simple-kriging",
    "href": "Computer_model.html#simple-kriging",
    "title": "5  Computer model",
    "section": "5.4 Simple Kriging",
    "text": "5.4 Simple Kriging\n\n5.4.1 Correctly guessed \\(\\mu\\) and \\(\\theta\\), estimated \\(\\sigma^2\\)\nLet us use simple kriging to develop a metamodel for the following one dimensional computer model, and visualize the fit, along with uncertainty.\n\n# Computer model\nf=function(x)\n  sin(30*(x-.9)^4)*cos(2*(x-.9))+(x-.9)/2\n\nn=10\nD=seq(0,1,length=n)\ny=f(D)\ncurve(f(x),from=0,to=1, ylim = c(-0.75, 0.55))\npoints(D,y,pch=4,cex=2)\nE=as.matrix(dist(D, diag=T, upper=T))\ntheta=100\nR=exp(-theta*E^2)\nmu=0\nI=diag(n)\nRinv=solve(R+10^(-10)*I)\ncoef=Rinv%*%(y-mu)\n\nbasis=function(h)\n  exp(-theta*h^2)\nr=function(x)\n{\n  vec=basis(x-D)\n  return(vec)\n}\n\nfhat=function(x)\n  mu+t(r(x))%*%coef\n\nsigma2=drop(1/n*t(y-mu)%*%Rinv%*%(y-mu))\nf.var=function(x)\n  sigma2*(1-t(r(x))%*%Rinv%*%r(x))\n\nN=1000\nu=seq(0,1,length=N)\ny.pred=y.low=y.high=numeric(N)\nfor(i in 1:N)\n{\n  y.pred[i]=fhat(u[i])\n  SD=sqrt(f.var(u[i]))\n  y.low[i]=y.pred[i]-2*SD\n  y.high[i]=y.pred[i]+2*SD\n}\n\nlines(u,y.pred,col=3,lwd=2)\nlines(u,y.low,lty=2,col=2,lwd=2)\nlines(u,y.high,lty=2,col=2,lwd=2)\n\n\n\n\n\n\n\ntest=u\n\nLet us find the root mean squared error (RMSE) on test data to evaluate the model accuracy in case of simple kriging.\n\n# RMSE\nsqrt(mean((apply(cbind(test),1,fhat)-f(test))^2))\n\n[1] 0.09318819\n\n\n\n\n5.4.2 Mis-specified \\(\\mu\\), correctly guessed \\(\\theta\\), estimated \\(\\sigma^2\\)\nNote that we assumed the mean \\(\\mu\\) to be \\(\\mu = 0\\). In case our guess for \\(\\mu\\) was incorrect, it would have resulted in a worse model. Let us visualize the fit, and confidence intervals, if our guess for \\(\\mu\\) was \\(\\mu = 100\\).\n\ncurve(f(x),from=0,to=1, ylim = c(-0.75, 0.55))\npoints(D,y,pch=4,cex=2)\nE=as.matrix(dist(D, diag=T, upper=T))\ntheta=100\nR=exp(-theta*E^2)\nmu=100\nI=diag(n)\nRinv=solve(R+10^(-10)*I)\ncoef=Rinv%*%(y-mu)\n\nbasis=function(h)\n  exp(-theta*h^2)\nr=function(x)\n{\n  vec=basis(x-D)\n  return(vec)\n}\n\nfhat=function(x)\n  mu+t(r(x))%*%coef\n\nsigma2=drop(1/n*t(y-mu)%*%Rinv%*%(y-mu))\nf.var=function(x)\n  sigma2*(1-t(r(x))%*%Rinv%*%r(x))\n\nN=1000\nu=seq(0,1,length=N)\ny.pred=y.low=y.high=numeric(N)\nfor(i in 1:N)\n{\n  y.pred[i]=fhat(u[i])\n  SD=sqrt(f.var(u[i]))\n  y.low[i]=y.pred[i]-2*SD\n  y.high[i]=y.pred[i]+2*SD\n}\n\nlines(u,y.pred,col=3,lwd=2)\nlines(u,y.low,lty=2,col=2,lwd=2)\nlines(u,y.high,lty=2,col=2,lwd=2)\n\n\n\n\n\n\n\n\nLet us find the root mean squared error (RMSE) on test data to evaluate the model accuracy in case of simple kriging, with mis-specified \\(\\mu\\).\n\n# RMSE\nsqrt(mean((apply(cbind(test),1,fhat)-f(test))^2))\n\n[1] 2.501507\n\n\nWe observe that the fit is much worse if our guess for the value of \\(\\mu\\) is very different from its true value. Thus, in such cases, we should estimate the mean \\(\\mu\\) from the data, or use ordinary kriging instead of simple kriging, as shown below.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Computer model</span>"
    ]
  },
  {
    "objectID": "Computer_model.html#ordinary-kriging",
    "href": "Computer_model.html#ordinary-kriging",
    "title": "5  Computer model",
    "section": "5.5 Ordinary Kriging",
    "text": "5.5 Ordinary Kriging\n\n5.5.1 Correctly guessed \\(\\theta\\), estimated \\(\\mu\\) and \\(\\sigma^2\\)\nIn ordinary kriging, we assume that we don’t know the value of \\(\\mu\\), and estimate it from the data.\n\none=rep(1,n)\nmu=drop(t(one)%*%Rinv%*%y/(t(one)%*%Rinv%*%one))\n\nThe estimated value of \\(\\mu\\) from the data is:\n\nmu\n\n[1] -0.1229525\n\n\nLet us use it to fit the ordinary kriging model.\n\ncoef=Rinv%*%(y-mu)\nsigma2=drop(1/n*t(y-mu)%*%Rinv%*%(y-mu))\nf.var=function(x)\n{ \n  fac=1/(one%*%Rinv%*%one)\n  val=sigma2*(1-t(r(x))%*%Rinv%*%r(x)+fac*(1-t(r(x))%*%Rinv%*%one)^2)\n  return(val)\n}\n\nfor(i in 1:N)\n{\n  y.pred[i]=fhat(u[i])\n  SD=sqrt(f.var(u[i]))\n  y.low[i]=y.pred[i]-2*SD\n  y.high[i]=y.pred[i]+2*SD\n}\ncurve(f(x),from=0,to=1, ylim= c(-0.75, 0.55))\npoints(D,y)\nlines(u,y.pred,col=3,lwd=2)\nlines(u,y.low,lty=2,col=2,lwd=2)\nlines(u,y.high,lty=2,col=2,lwd=2)\n\n\n\n\n\n\n\n\nLet us find the root mean squared error (RMSE) on test data to evaluate the model accuracy in case of ordinary kriging, where \\(\\mu\\) is estimated from the data.\n\n# RMSE\nsqrt(mean((apply(cbind(test),1,fhat)-f(test))^2))\n\n[1] 0.09235139\n\n\nNote that the model is more accurate that simple kriging as \\(\\mu\\) is estimated from the data, instead of being guessed.\n\n\n5.5.2 Unreasonably high \\(\\theta\\), estimated \\(\\mu\\) and \\(\\sigma^2\\)\nIn the above code on simple kriging, we have used the maximum likelihood estimates of \\(\\mu\\), and \\(\\sigma^2\\). However, we guessed the value of \\(\\theta\\). If \\(\\theta\\) is guessed incorrectly, then the model accuracy may deteriorate. Let us fit the model for \\(\\theta\\) = 1000.\n\ntheta=1000\nR=exp(-theta*E^2)\none=rep(1,n)\nmu=drop(t(one)%*%Rinv%*%y/(t(one)%*%Rinv%*%one))\nI=diag(n)\nRinv=solve(R+10^(-10)*I)\ncoef=Rinv%*%(y-mu)\nsigma2=drop(1/n*t(y-mu)%*%Rinv%*%(y-mu))\n\nfor(i in 1:N)\n{\n  y.pred[i]=fhat(u[i])\n  SD=sqrt(f.var(u[i]))\n  y.low[i]=y.pred[i]-2*SD\n  y.high[i]=y.pred[i]+2*SD\n}\ncurve(f(x),from=0,to=1, ylim= c(-0.75, 0.55))\npoints(D,y)\nlines(u,y.pred,col=3,lwd=2)\nlines(u,y.low,lty=2,col=2,lwd=2)\nlines(u,y.high,lty=2,col=2,lwd=2)\n\n\n\n\n\n\n\n\nNote that as \\(\\theta\\) increases, the model tends to be pulled towards the mean, as the correlation between the neighboring response values reduces.\nLet us find the root mean squared error (RMSE) on test data to evaluate the model accuracy in case of simple kriging, with a high value of\\(\\theta\\).\n\n# RMSE\nsqrt(mean((apply(cbind(test),1,fhat)-f(test))^2))\n\n[1] 0.178251\n\n\nAs expected, the model accuracy has deteriorated for an unreasonably high value of the correlation parameter \\(\\theta\\).\n\n\n5.5.3 Unreasonably low \\(\\theta\\), estimated \\(\\mu\\) and \\(\\sigma^2\\)\nLet us fit the model for \\(\\theta\\) = 1.\n\ntheta=1\nR=exp(-theta*E^2)\none=rep(1,n)\nmu=drop(t(one)%*%Rinv%*%y/(t(one)%*%Rinv%*%one))\nI=diag(n)\nRinv=solve(R+10^(-7)*I)\ncoef=Rinv%*%(y-mu)\nsigma2=drop(1/n*t(y-mu)%*%Rinv%*%(y-mu))\n\nfor(i in 1:N)\n{\n  y.pred[i]=fhat(u[i])\n  SD=sqrt(f.var(u[i]))\n  y.low[i]=y.pred[i]-2*SD\n  y.high[i]=y.pred[i]+2*SD\n}\ncurve(f(x),from=0,to=1, ylim= c(-0.75, 0.55))\npoints(D,y)\nlines(u,y.pred,col=3,lwd=2)\nlines(u,y.low,lty=2,col=2,lwd=2)\nlines(u,y.high,lty=2,col=2,lwd=2)\n\n\n\n\n\n\n\n\nNote that as \\(\\theta\\) decreases, the model becomes overly smooth, as the correlation between the neighboring response values is high.\nLet us find the root mean squared error (RMSE) on test data to evaluate the model accuracy in case of simple kriging, with a low value of \\(\\theta\\).\n\n# RMSE\nsqrt(mean((apply(cbind(test),1,fhat)-f(test))^2))\n\n[1] 0.139509\n\n\nAs expected, the model accuracy has deteriorated for an unreasonably low value of the correlation parameter \\(\\theta\\).",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Computer model</span>"
    ]
  },
  {
    "objectID": "Computer_model.html#kriging-with-mle",
    "href": "Computer_model.html#kriging-with-mle",
    "title": "5  Computer model",
    "section": "5.6 Kriging with MLE",
    "text": "5.6 Kriging with MLE\nIn general, in case of unknown value of the parameters, instead of guessing their values, it is better to use their maximum likelihood estimates from the data. The R function mlegp() can be used to do that.\n\nlibrary(mlegp)\n\n# Fit the model to the data\nmodel &lt;- mlegp(D, y)\n\nno reps detected - nugget will not be estimated\n\n========== FITTING GP # 1 ==============================\nrunning simplex # 1...\n...done\n...simplex #1 complete, loglike = 0.546783 (convergence)\nrunning simplex # 2...\n...done\n...simplex #2 complete, loglike = 0.546783 (convergence)\nrunning simplex # 3...\n...done\n...simplex #3 complete, loglike = 0.546783 (convergence)\nrunning simplex # 4...\n...done\n...simplex #4 complete, loglike = 0.546783 (convergence)\nrunning simplex # 5...\n...done\n...simplex #5 complete, loglike = 0.546783 (convergence)\n\nusing L-BFGS method from simplex #3...\n    iteration: 1,loglike = 0.546783\n...L-BFGS method complete\n\nMaximum likelihood estimates found, log like =  0.546783\ncreating gp object......done\n\n\nThe parameter estimates are as follows:\nMaximum likelihood estimate (MLE) of the mean \\(\\mu\\) is:\n\nmodel$mu\n\n            [,1]\n [1,] -0.1319677\n [2,] -0.1319677\n [3,] -0.1319677\n [4,] -0.1319677\n [5,] -0.1319677\n [6,] -0.1319677\n [7,] -0.1319677\n [8,] -0.1319677\n [9,] -0.1319677\n[10,] -0.1319677\n\n\nMaximum likelihood estimate (MLE) of the variance \\(\\sigma^2\\) is:\n\nmodel$sig2\n\n[1] 0.07437114\n\n\nMaximum likelihood estimate (MLE) of the correlation parameter \\(\\theta\\) is:\n\nmodel$beta\n\n[1] 54.08925\n\n\nFor making predictions, the function predict.gp() is used, and the observations for prediction must be specified as a matrix or a dataframe.\n\nmodel_predictions &lt;- predict.gp(model, matrix(test,1000,1))\n\nThe RMSE is:\n\nsqrt(mean((apply(cbind(test),1,fhat)-model_predictions)^2))\n\n[1] 0.100242\n\n\nThe confidence interval is also returned using the argument se.fit = TRUE in the predict.gp() function.\n\nmodel_predictions_with_se &lt;- predict.gp(model, matrix(test,1000,1), se.fit = TRUE)\n\nLet us plot the model and the confidence interval:\n\ncurve(f(x),from=0,to=1, ylim= c(-0.75, 0.55))\npoints(D,y)\nmean_prediction &lt;- model_predictions_with_se$fit\nse_prediction &lt;- model_predictions_with_se$se.fit\nlines(u,mean_prediction,col=3,lwd=2)\nlines(u,mean_prediction - 2*se_prediction,lty=2,col=2,lwd=2)\nlines(u,mean_prediction + 2*se_prediction,lty=2,col=2,lwd=2)",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Computer model</span>"
    ]
  },
  {
    "objectID": "computer_experiment_designs.html",
    "href": "computer_experiment_designs.html",
    "title": "6  Computer experiment designs",
    "section": "",
    "text": "6.1 Maximin design\nLet us generate a 4-run maximin design in \\([0, 1]^2\\) for \\(p=2\\).\nWe will use the maximin() function of the maximin library\nlibrary(maximin)\nFor generating a maximin design, we need to specify an initial design, and the function will add points to the design to develop a design that is as close to a maximin design as possible.\nLet us add the origin as a point in the initial design.\nD_initial &lt;- matrix(c(0,0),1,2)\nset.seed(3)\nD &lt;- maximin(n = 3, p=2, Xorig = D_initial)$Xf\npar(pty=\"s\") # Making a square plot (maintaining aspect ratio)\nplot(D, pch = 16)\nAs expected, the design is the four corners of the feasible design space as the minimum distance between any two design points is the largest in this case.\nNote that the algorithm that maximizes the minimum distance between points is an iterative algorithm that does not guarantee to find the global optimum. The more iterations it runs, the higher is the likelihood to find the global optimum. One can increase the number of iterations of the optimization algorithm, and / or execute the code multiple times, and compare the minimum distance between points to obtain a better maximin design.\nLet us attempt to obtain a 5-run maximin design in \\([0, 1]^2\\) for \\(p=2\\).\nset.seed(3)\nD &lt;- maximin(n = 4, p=2, Xorig = D_initial)\npar(pty=\"s\")\nplot(D$Xf, pch = 16)\nLet us compute the maximin distance for this design.\n# Function to obtain the minimum distance between \n# a point and a design\nminimum_distance &lt;- function(x, D)\n{\n  sq_distances &lt;- (colSums((t(D)- x)^2))\n  sq_distances[sq_distances == 0] &lt;- Inf\n  return(sqrt(min(sq_distances)))\n}\nmin(apply(D$Xf, 1, minimum_distance, D = D$Xf))\n\n[1] 0.4713757\nLet us increase the iterations of the maximin() function to see if that leads to a better design.\nset.seed(3)\nD &lt;- maximin(n = 4, p=2, Xorig = D_initial, T = 100)\npar(pty=\"s\")\nplot(D$Xf, pch = 16)\n\n\n\n\n\n\n\nmin(apply(D$Xf, 1, minimum_distance, D = D$Xf))\n\n[1] 0.4714045\nIncreasing the number of iteration did not help. We can increase the iterations further. However, let us try to change the seed.\nset.seed(4)\nD &lt;- maximin(n = 4, p=2, Xorig = D_initial, T = 100)\npar(pty=\"s\")\nplot(D$Xf, pch = 16)\n\n\n\n\n\n\n\nmin(apply(D$Xf, 1, minimum_distance, D = D$Xf))\n\n[1] 0.5962154\nWe got a better design, as the minimum distance between the points is higher. Let us increase the number of iterations to see if the design gets better.\nset.seed(4)\nD &lt;- maximin(n = 4, p=2, Xorig = D_initial, T = 200)\npar(pty=\"s\")\nplot(D$Xf, pch = 16)\n\n\n\n\n\n\n\nmin(apply(D$Xf, 1, minimum_distance, D = D$Xf))\n\n[1] 0.6469957\nThe design gets better with increased iterations. Let us further increase the iterations.\nset.seed(4)\nD &lt;- maximin(n = 4, p=2, Xorig = D_initial, T = 400)\npar(pty=\"s\")\nplot(D$Xf, pch = 16)\n\n\n\n\n\n\n\nmin(apply(D$Xf, 1, minimum_distance, D = D$Xf))\n\n[1] 0.6468853\nThe design did not get better. Let us change the seed again.\nset.seed(5)\nD &lt;- maximin(n = 4, p=2, Xorig = D_initial, T = 400)\npar(pty=\"s\")\nplot(D$Xf, pch = 16)\n\n\n\n\n\n\n\nmin(apply(D$Xf, 1, minimum_distance, D = D$Xf))\n\n[1] 0.4714045\nThe design did not get better. Let us change use a for() loop to try a lot of different seeds.\nmin_dist &lt;- rep(0, 20)\nfor(s in 1:20)\n{\n  set.seed(s)\n  D &lt;- maximin(n = 4, p=2, Xorig = D_initial, T = 100)\n  min_dist[s] &lt;- min(apply(D$Xf, 1, minimum_distance, D = D$Xf))\n}\nThe seed for the best design, i.e., the design with the maximum minimum distance between points is:\nwhich.max(min_dist)\n\n[1] 12\nLet us plot the best design:\nset.seed(12)\nD &lt;- maximin(n = 4, p=2, Xorig = D_initial, T = 100)\npar(pty=\"s\")\nplot(D$Xf, pch = 16)\n\n\n\n\n\n\n\nmin(apply(D$Xf, 1, minimum_distance, D = D$Xf))\n\n[1] 0.7071068",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Computer experiment designs</span>"
    ]
  },
  {
    "objectID": "computer_experiment_designs.html#maximin-lhd",
    "href": "computer_experiment_designs.html#maximin-lhd",
    "title": "6  Computer experiment designs",
    "section": "6.2 Maximin LHD",
    "text": "6.2 Maximin LHD\nLet us generate a 4-run maximin LHD in \\([0, 1]^2\\) for \\(p=2\\).\nWe will use the maximinLHS() function of the lhs library\n\nlibrary(lhs)\n\n\nset.seed(1)\nD &lt;- maximinLHS(4, 2)\npar(pty=\"s\")\nplot(D, xlim = c(0, 1), ylim = c(0, 1), pch = 16)\n\n\n\n\n\n\n\nmin(apply(D, 1, minimum_distance, D = D))\n\n[1] 0.4907556\n\n\nNote that the design has 4 distinct projections for each of the two factors.\nAs with maximin designs, the function maximinLHS() is an iterative algorithm that does not guarantee the global optimum. Thus, we need to run the algorithm with different seeds to find a good MaximinLHD.\n\nmin_dist &lt;- rep(0, 20)\nfor(s in 1:20)\n{\n  set.seed(s)\n  D &lt;- maximinLHS(n = 4, k=2)\n  min_dist[s] &lt;- min(apply(D, 1, minimum_distance, D = D))\n}\n\nThe seed for the best MaximinLHD is:\n\nwhich.max(min_dist)\n\n[1] 6\n\n\nLet us plot the best MaximinLHD.\n\nset.seed(6)\nD &lt;- maximinLHS(4, 2)\npar(pty=\"s\")\nplot(D, xlim = c(0, 1), ylim = c(0, 1), pch = 16)\n\n\n\n\n\n\n\nmin(apply(D, 1, minimum_distance, D = D))\n\n[1] 0.537882\n\n\n\n6.2.1 MmLHD: Weakness\nMaximinLHD have good projections in a single dimension, but projection properties in \\(2, 3, …, p−1\\) dimensions may not be good.\nLet us compare the projections of a 20-run 2-dimensional design with that of a 20-run 10-dimensional design on a two-dimensional subspace.\nThe best 20-run MmLHD for \\(p=2\\) is with seed:\n\nmin_dist &lt;- rep(0, 10000)\nfor(s in 1:10000)\n{\n  set.seed(s)\n  D &lt;- maximinLHS(n = 20, k=2)\n  min_dist[s] &lt;- min(apply(D, 1, minimum_distance, D = D))\n}\nwhich.max(min_dist)\n\n[1] 7312\n\n\nThe best 20-run MmLHD for \\(p = 10\\) is with seed:\n\nmin_dist &lt;- rep(0, 10000)\nfor(s in 1:10000)\n{\n  set.seed(s)\n  D &lt;- maximinLHS(n = 20, k=10)\n  min_dist[s] &lt;- min(apply(D, 1, minimum_distance, D = D))\n}\nwhich.max(min_dist)\n\n[1] 2541\n\n\nLet us visualize the projections of the 20 run 2-dimensional MmLHD, and the 20-run 10-dimensional MmLHD on a 2-dimensional subspace, where the subspace is the subspace of the first two predictors.\n\nset.seed(7312)\nD &lt;- maximinLHS(20, 2)\npar(mfrow = c(1,2))\npar(pty=\"s\")\nplot(D, pch = 16, main = \"MmLHD(n = 20, p = 2)\")\nset.seed(2541)\nD &lt;- maximinLHS(20, 10)\npar(pty=\"s\")\nplot(D[,1:2], pch = 16, main = \"MmLHD(n = 20, p = 10)\")\n\n\n\n\n\n\n\n\nWe observe that the projections of the 10-dimensional MmLHD are not as space-filling in the 2-dimensional subspace as the projections of the 2-dimensional MmLHD.\nThis issue motivates us to present a design that maximizes space-filling of the design projections in all multidimensional sub-spaces.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Computer experiment designs</span>"
    ]
  },
  {
    "objectID": "computer_experiment_designs.html#maxprolhd-maxpro",
    "href": "computer_experiment_designs.html#maxprolhd-maxpro",
    "title": "6  Computer experiment designs",
    "section": "6.3 MaxProLHD & MaxPro",
    "text": "6.3 MaxProLHD & MaxPro\nLet us generate a 20-run MaxProLHD and MaxPro designs in \\([0, 1]^{10}\\) for \\(p=10\\).\nWe will use the MaxProLHD() and MaxPro() functions of the MaxPro library\n\nlibrary(MaxPro)\n\nLet us find the best MaxProLHD design among 100 different MaxProLHDs. The seed that corresponds to the least MaxPro criterion is:\n\nmaxpro_criterion &lt;- rep(0, 100)\nfor(s in 1:100)\n{\n  set.seed(s)\n  Dm &lt;- MaxProLHD(n = 20, p = 10)\n  maxpro_criterion[s] &lt;- Dm$measure\n}\nwhich.min(maxpro_criterion)\n\n[1] 26\n\n\nWe will use the seed to visualize the projections of a 20-run MaxProLHD and MaxPro design in 10 dimensions on a 2-dimensional subspace.\n\nset.seed(26)\nD1 &lt;- MaxProLHD(20, 10)$Design\nD2 &lt;- MaxPro(D1)$Design\npar(mfrow = c(1,2))\npar(pty=\"s\")\nplot(D1[,1:2], pch = 16, main = \"MaxProLHD(n = 20, p = 10)\")\npar(pty=\"s\")\nplot(D2[,1:2], pch = 16, main = \"MaxPro(n = 20, p = 10)\")\n\n\n\n\n\n\n\n\nLet us compare the 2-dimensional projections of the 20-run MaximinLHD with MaxPro design.\n\npar(mfrow = c(1,2))\npar(pty=\"s\")\nplot(D[,1:2], pch = 16, main = \"MmLHD(n = 20, p = 10)\")\npar(pty=\"s\")\nplot(D2[,1:2], pch = 16, main = \"MaxPro(n = 20, p = 10)\")\n\n\n\n\n\n\n\n\nNote that the projections of the MaxPro design in the 2-dimensional subspace are more space-filling than that of the MaximinLHD, as expected.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Computer experiment designs</span>"
    ]
  },
  {
    "objectID": "computer_experiment_designs.html#low-descrepancy-sequences",
    "href": "computer_experiment_designs.html#low-descrepancy-sequences",
    "title": "6  Computer experiment designs",
    "section": "6.4 Low descrepancy sequences",
    "text": "6.4 Low descrepancy sequences\nWe will use the library randtoolbox to generate low-discrepancy sequences.\n\nlibrary(randtoolbox)\n\nLoading required package: rngWELL\n\n\nThis is randtoolbox. For an overview, type 'help(\"randtoolbox\")'.\n\n\n\n6.4.1 Sobol\nLet us visualize a 100-run 2-dimensional Sobol sequence.\n\nD_sobol &lt;- sobol(100, 2)\npar(pty=\"s\")\nplot(D_sobol)\n\n\n\n\n\n\n\n\nAs we can see, low descrepancy sequences minimize the difference between the proportion of points in a subregion and the proportion of the volume of that subregion. However, they do not necessarily have good projections. The same is true for the Halton sequence in the next section.\n\n\n6.4.2 Halton\nLet us visualize a 100-run 2-dimensional Halton sequence.\n\nD_halton &lt;- halton(100, 2)\npar(pty=\"s\")\nplot(D_halton)",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Computer experiment designs</span>"
    ]
  },
  {
    "objectID": "computer_experiment_designs.html#comparison-of-time-taken-by-each-design",
    "href": "computer_experiment_designs.html#comparison-of-time-taken-by-each-design",
    "title": "6  Computer experiment designs",
    "section": "6.5 Comparison of time taken by each design",
    "text": "6.5 Comparison of time taken by each design\nLet us compare the time taken by each design to generate a 50-run design in 50 dimensions.\n\nn=50\ntime_measure &lt;- matrix(0, 10, 6)\n\n# Sobol\ntime_start &lt;- proc.time()[3]\nfor(i in 1:10)\n{\n  D &lt;- sobol(n, 5)\n  time_measure[i,1] &lt;- proc.time()[3] - time_start\n}\n\n# Halton\ntime_start &lt;- proc.time()[3]\nfor(i in 1:10)\n{\n  D &lt;- halton(n, 5)\n  time_measure[i,2] &lt;- proc.time()[3] - time_start\n}\n\n# Maximin\ntime_start &lt;- proc.time()[3]\nfor(i in 1:10)\n{\n  D_initial &lt;- matrix(rep(0, 5),1,5)\n  D &lt;- maximin(n = n-1, p=5, Xorig = D_initial)$Xf\n  time_measure[i,3] &lt;- proc.time()[3] - time_start\n}\n\n# MaximinLHD\ntime_start &lt;- proc.time()[3]\nfor(i in 1:10)\n{\n  D &lt;- maximinLHS(n = n, k=5)\n  time_measure[i,4] &lt;- proc.time()[3] - time_start\n}\n\n# MaxProLHD\ntime_start &lt;- proc.time()[3]\nfor(i in 1:10)\n{\n  Dm &lt;- MaxProLHD(n = n, p = 5)\n  time_measure[i,5] &lt;- proc.time()[3] - time_start\n}\n\n# MaxPro\ntime_start &lt;- proc.time()[3]\nfor(i in 1:10)\n{\n  D1 &lt;- MaxProLHD(n = n, p = 5)$Design\n  d2 &lt;- MaxPro(D1)$Design\n  time_measure[i,6] &lt;- proc.time()[3] - time_start\n}\nboxplot(time_measure, names = c(\"Sobol\", \"Halton\", \"Maximin\", \"MmLHD\", \"MProLHD\", \n\"MaxPro\"), ylab = \"Time (in seconds)\")\n\n\n\n\n\n\n\n\nMaximin design takes the longest time as there are no constraints in the design criterion.\nLet us remove the maximin design from the plot to visualize the comparison of other designs more clearly.\n\nboxplot(time_measure[,-3], names = c(\"Sobol\", \"Halton\", \"MmLHD\", \"MProLHD\", \n\"MaxPro\"), ylab = \"Time (in seconds)\")\n\n\n\n\n\n\n\n\nMaxProLHD and MaxPro take longer time than MmLHD as the MaxPro criterion is optimized.\nLet us remove MaxProLHD and MaxPro from the plot to compare the time taken by MmLHD with low descrepancy sequences.\n\nboxplot(time_measure[,-c(3,5:6)], names = c(\"Sobol\", \"Halton\", \"MmLHD\"), \n        ylab = \"Time (in seconds)\")\n\n\n\n\n\n\n\n\nAs expected, MmLHD takes more time as it includes optimization of the MmLHD criterion.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Computer experiment designs</span>"
    ]
  }
]