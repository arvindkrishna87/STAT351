# Computer model

Assume that the true computer model is the following:

```{r}
curve(1/(1+x^2), from=-4, to=4, ylim=c(-.7,1))
```

We will consider potential methods to replace the computer model above with a metamodel or a surrogate model.

## Polynomial interpolation

Below is an example of a $7$-point polynomial interpolator.

```{r}
curve(1/(1+x^2), from=-4, to=4, ylim=c(-.7,1))
x=seq(-4,4,length=7)
y=1/(1+x^2)
X=as.matrix(cbind(1,x,x^2,x^3,x^4,x^5,x^6))
a=solve(X,y)
u=seq(-4,4,length=100)
yhat=u
for(i in 1:100)
  yhat[i]=sum(c(1,u[i],u[i]^2,u[i]^3,u[i]^4,u[i]^5,u[i]^6)*a)
lines(u,yhat, col=2, lty=2, lwd = 2)
points(x,y,col=2)
```

Notice that the polynomial interpolation model tends to be unstable near the edges, this is called Runge's phenomenon.

The instability increases as the degree of the polynomial increases. Consider the same example if 9 equally-spaced points are considered, instead of 7.

```{r}
curve(1/(1+x^2), from=-4, to=4, ylim=c(-.7,1))
x=seq(-4,4,length=9)
y=1/(1+x^2)
X=as.matrix(cbind(1,x,x^2,x^3,x^4,x^5,x^6,x^7,x^8))
a=solve(X,y)
u=seq(-4,4,length=100)
yhat=u
for(i in 1:100)
  yhat[i]=sum(c(1,u[i],u[i]^2,u[i]^3,u[i]^4,u[i]^5,u[i]^6, u[i]^7,u[i]^8)*a)
lines(u,yhat, col='blue', lty=3, lwd = 2)
points(x,y,col="blue")
```

## Splines

Let us consider splines as the metamodel to replace the same computer model considered above.

```{r}
curve(1/(1+x^2), from=-4, to=4, ylim=c(-.7,1))

# Splines with 7 points

x=seq(-4,4,length=7)
y=1/(1+x^2)
points(x,y,col=2)
a=splinefun(x,y,method="natural")
curve(a,add=T,col=2, lwd = 2)

# Splines with 9 points

x=seq(-4,4,length=9)
y=1/(1+x^2)
points(x,y, pch=3,col=3)
a=splinefun(x,y,method="natural")
curve(a,add=T,col=3, lty=2, lwd = 2)
```

We observe that splines fit the points smoothly, which is desired. However, splines do not scale up and are difficult to fit in case of higher dimensions.


## Random functions

We will now adopt a statistical approach to replace the computer model with a metamodel, where we view the deterministic computer model as a realization from a stochastic process.

Let us consider quadratic functions as the random functions to develop the metamodel.

```{r}
x=seq(-1,1,length=10)
b=rnorm(3)
y=b[1]+b[2]*x+b[3]*x^2
plot(x,y,"l", ylim=c(-3,3))
b=rnorm(3)
y=b[1]+b[2]*x+b[3]*x^2
lines(x,y,"l")
for(i in 1:10)
{b=rnorm(3)
y=b[1]+b[2]*x+b[3]*x^2
lines(x,y,"l",col=i)}
```

We want a more flexible random function, so that a flexible computer model could correspond to a realization from that random function.

Suppose $y(x_i) \sim N(0, \sigma^2)$. Let us plot a realization of this model for $\sigma^2 = 1$.

```{r}
N=100
x=seq(-1,1,length=N)
y=rnorm(N)
plot(x,y,"l", ylim=c(-3,3))
```

Although the function does seem to be flexible, it is not smooth. We want to have a smooth function, which is also flexible, as the metamodel.

The reason why the above realization of a computer model is not smooth is because the response values are not correlated. For smooth functions, adjacent points $y(x)$ and $y(x + \Delta)$ should be positively correlated.

Let us introduce correlation between response values to make the realizations of the computer models more smooth.

Let us assume that the response values have an underlying multivariate normal distribution, with a covariance matrix that is not the diagonal matrix.

For example, consider the covariance matrix $R$. where:

$$R = \exp\{-\theta(x_i-x_j)^2\}_{100 \times 100}$$
In the above correlation function, we can control the flexibility by changing the value of $\theta$. The higher the value of $\theta$, the lesser is the correlation between the response values, for a given distance between inputs, and the higher is the flexibility of the function. Thus, this correlation function can provide the desired level of flexibility while also providing smoothness in the function. 

Then, 

$$y \sim N(0, R)$$
Let us visualize 10 realizations from the above random function to see the smoothness.

If $z \sim N(0, 1)$, then $y = R^{(1/2)}z $.

Proof:
Given $y = R^{1/2}z$,

$E(y) = R^{1/2}E(z) = 0$

$Var(y) = Var(R^{1/2}z)$

$\implies Var(y) = R^{1/2}Var(z)R^{1/2})$

$\implies Var(y) = R^{1/2}IR^{1/2})$

$\implies Var(y) = R$

```{r}
E=as.matrix(dist(x, diag=T, ,upper=T))

# Correlation matrix R
R=exp(-0.5*E^2)
eig=eigen(R)
R=eig$vec%*%diag(sqrt(eig$val+10^(-10)))%*%t(eig$vec)
z=rnorm(N)
y=R%*%z
plot(x,y,"l", ylim=c(-3,3))
for(i in 1:10)
  lines(x,R%*%rnorm(N),col=i)
```




